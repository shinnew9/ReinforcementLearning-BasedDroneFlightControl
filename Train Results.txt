---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.5      |
|    mean_reward     | 54       |
| time/              |          |
|    total timesteps | 116000   |
---------------------------------
Eval num_timesteps=116500, episode_reward=124.40 +/- 13.69
Episode length: 10.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 124      |
| time/              |          |
|    total timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 8.9      |
|    ep_rew_mean     | -16.6    |
| time/              |          |
|    fps             | 11       |
|    iterations      | 57       |
|    time_elapsed    | 10587    |
|    total_timesteps | 116736   |
---------------------------------
Eval num_timesteps=117000, episode_reward=57.89 +/- 74.17
Episode length: 9.50 +/- 0.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9.5         |
|    mean_reward          | 57.9        |
| time/                   |             |
|    total timesteps      | 117000      |
| train/                  |             |
|    approx_kl            | 0.059954394 |
|    clip_fraction        | 0.331       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.836      |
|    explained_variance   | 0.159       |
|    learning_rate        | 0.0003      |
|    loss                 | 819         |
|    n_updates            | 570         |
|    policy_gradient_loss | 0.0312      |
|    value_loss           | 2.51e+03    |
-----------------------------------------
Eval num_timesteps=117500, episode_reward=10.79 +/- 87.86
Episode length: 9.00 +/- 1.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 10.8     |
| time/              |          |
|    total timesteps | 117500   |
---------------------------------
Eval num_timesteps=118000, episode_reward=49.57 +/- 79.15
Episode length: 9.50 +/- 0.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.5      |
|    mean_reward     | 49.6     |
| time/              |          |
|    total timesteps | 118000   |
---------------------------------
Eval num_timesteps=118500, episode_reward=70.27 +/- 70.63
Episode length: 9.50 +/- 0.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.5      |
|    mean_reward     | 70.3     |
| time/              |          |
|    total timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.14     |
|    ep_rew_mean     | 21       |
| time/              |          |
|    fps             | 11       |
|    iterations      | 58       |
|    time_elapsed    | 10775    |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=119000, episode_reward=67.06 +/- 91.56
Episode length: 9.50 +/- 0.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 9.5        |
|    mean_reward          | 67.1       |
| time/                   |            |
|    total timesteps      | 119000     |
| train/                  |            |
|    approx_kl            | 0.09710493 |
|    clip_fraction        | 0.372      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.755     |
|    explained_variance   | 0.365      |
|    learning_rate        | 0.0003     |
|    loss                 | 856        |
|    n_updates            | 580        |
|    policy_gradient_loss | 0.0637     |
|    value_loss           | 2.41e+03   |
----------------------------------------
Eval num_timesteps=119500, episode_reward=51.40 +/- 69.44
Episode length: 9.50 +/- 0.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.5      |
|    mean_reward     | 51.4     |
| time/              |          |
|    total timesteps | 119500   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-32.23 +/- 79.37
Episode length: 8.50 +/- 0.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.5      |
|    mean_reward     | -32.2    |
| time/              |          |
|    total timesteps | 120000   |
---------------------------------
Eval num_timesteps=120500, episode_reward=-1.54 +/- 86.88
Episode length: 8.75 +/- 1.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.75     |
|    mean_reward     | -1.54    |
| time/              |          |
|    total timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.42     |
|    ep_rew_mean     | 50.2     |
| time/              |          |
|    fps             | 11       |
|    iterations      | 59       |
|    time_elapsed    | 10962    |
|    total_timesteps | 120832   |
---------------------------------
Eval num_timesteps=121000, episode_reward=44.65 +/- 88.03
Episode length: 9.50 +/- 0.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 9.5        |
|    mean_reward          | 44.6       |
| time/                   |            |
|    total timesteps      | 121000     |
| train/                  |            |
|    approx_kl            | 0.12771617 |
|    clip_fraction        | 0.412      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.788     |
|    explained_variance   | 0.209      |
|    learning_rate        | 0.0003     |
|    loss                 | 821        |
|    n_updates            | 590        |
|    policy_gradient_loss | 0.0701     |
|    value_loss           | 2.32e+03   |
----------------------------------------
Eval num_timesteps=121500, episode_reward=125.56 +/- 25.92
Episode length: 10.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 126      |
| time/              |          |
|    total timesteps | 121500   |
---------------------------------
Eval num_timesteps=122000, episode_reward=11.93 +/- 88.20
Episode length: 9.00 +/- 1.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 11.9     |
| time/              |          |
|    total timesteps | 122000   |
---------------------------------
Eval num_timesteps=122500, episode_reward=97.12 +/- 32.77
Episode length: 10.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 97.1     |
| time/              |          |
|    total timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.04     |
|    ep_rew_mean     | 12.5     |
| time/              |          |
|    fps             | 11       |
|    iterations      | 60       |
|    time_elapsed    | 11149    |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=123000, episode_reward=68.51 +/- 85.46
Episode length: 9.50 +/- 0.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 9.5        |
|    mean_reward          | 68.5       |
| time/                   |            |
|    total timesteps      | 123000     |
| train/                  |            |
|    approx_kl            | 0.07493666 |
|    clip_fraction        | 0.351      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.756     |
|    explained_variance   | 0.351      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.05e+03   |
|    n_updates            | 600        |
|    policy_gradient_loss | 0.0318     |
|    value_loss           | 2.13e+03   |
----------------------------------------
Eval num_timesteps=123500, episode_reward=68.87 +/- 83.72
Episode length: 9.50 +/- 0.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.5      |
|    mean_reward     | 68.9     |
| time/              |          |
|    total timesteps | 123500   |
---------------------------------
Eval num_timesteps=124000, episode_reward=97.38 +/- 14.20
Episode length: 10.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 97.4     |
| time/              |          |
|    total timesteps | 124000   |
---------------------------------
Eval num_timesteps=124500, episode_reward=115.35 +/- 27.27
Episode length: 10.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 115      |
| time/              |          |
|    total timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.3      |
|    ep_rew_mean     | 31.4     |
| time/              |          |
|    fps             | 11       |
|    iterations      | 61       |
|    time_elapsed    | 11337    |
|    total_timesteps | 124928   |
---------------------------------
Eval num_timesteps=125000, episode_reward=64.76 +/- 78.87
Episode length: 9.50 +/- 0.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 9.5        |
|    mean_reward          | 64.8       |
| time/                   |            |
|    total timesteps      | 125000     |
| train/                  |            |
|    approx_kl            | 0.07657258 |
|    clip_fraction        | 0.33       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.721     |
|    explained_variance   | 0.331      |
|    learning_rate        | 0.0003     |
|    loss                 | 744        |
|    n_updates            | 610        |
|    policy_gradient_loss | 0.0337     |
|    value_loss           | 1.96e+03   |
----------------------------------------
Eval num_timesteps=125500, episode_reward=18.37 +/- 87.94
Episode length: 9.25 +/- 0.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.25     |
|    mean_reward     | 18.4     |
| time/              |          |
|    total timesteps | 125500   |
---------------------------------
Eval num_timesteps=126000, episode_reward=94.08 +/- 28.19
Episode length: 10.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 94.1     |
| time/              |          |
|    total timesteps | 126000   |
---------------------------------
Eval num_timesteps=126500, episode_reward=61.10 +/- 84.37
Episode length: 9.50 +/- 0.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.5      |
|    mean_reward     | 61.1     |
| time/              |          |
|    total timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.28     |
|    ep_rew_mean     | 35.1     |
| time/              |          |
|    fps             | 11       |
|    iterations      | 62       |
|    time_elapsed    | 11524    |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=127000, episode_reward=118.24 +/- 34.59
Episode length: 10.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 10         |
|    mean_reward          | 118        |
| time/                   |            |
|    total timesteps      | 127000     |
| train/                  |            |
|    approx_kl            | 0.10061988 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.708     |
|    explained_variance   | 0.294      |
|    learning_rate        | 0.0003     |
|    loss                 | 985        |
|    n_updates            | 620        |
|    policy_gradient_loss | 0.0443     |
|    value_loss           | 2.05e+03   |
----------------------------------------
Eval num_timesteps=127500, episode_reward=37.07 +/- 72.29
Episode length: 9.50 +/- 0.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.5      |
|    mean_reward     | 37.1     |
| time/              |          |
|    total timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=48.99 +/- 87.91
Episode length: 8.75 +/- 2.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.75     |
|    mean_reward     | 49       |
| time/              |          |
|    total timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=77.34 +/- 96.11
Episode length: 9.50 +/- 0.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.5      |
|    mean_reward     | 77.3     |
| time/              |          |
|    total timesteps | 128500   |
---------------------------------
Eval num_timesteps=129000, episode_reward=123.57 +/- 12.59
Episode length: 10.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 124      |
| time/              |          |
|    total timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.5      |
|    ep_rew_mean     | 58.5     |
| time/              |          |
|    fps             | 11       |
|    iterations      | 63       |
|    time_elapsed    | 11717    |
|    total_timesteps | 129024   |
---------------------------------
Eval num_timesteps=129500, episode_reward=70.21 +/- 23.62
Episode length: 10.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 10          |
|    mean_reward          | 70.2        |
| time/                   |             |
|    total timesteps      | 129500      |
| train/                  |             |
|    approx_kl            | 0.087851174 |
|    clip_fraction        | 0.329       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.701      |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.0003      |
|    loss                 | 572         |
|    n_updates            | 630         |
|    policy_gradient_loss | 0.0551      |
|    value_loss           | 1.81e+03    |
-----------------------------------------
Eval num_timesteps=130000, episode_reward=29.46 +/- 69.92
Episode length: 9.00 +/- 1.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 29.5     |
| time/              |          |
|    total timesteps | 130000   |
---------------------------------
Eval num_timesteps=130500, episode_reward=61.96 +/- 84.60
Episode length: 9.50 +/- 0.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.5      |
|    mean_reward     | 62       |
| time/              |          |
|    total timesteps | 130500   |
---------------------------------
Eval num_timesteps=131000, episode_reward=10.75 +/- 92.36
Episode length: 8.75 +/- 1.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.75     |
|    mean_reward     | 10.8     |
| time/              |          |
|    total timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.28     |
|    ep_rew_mean     | 37.2     |
| time/              |          |
|    fps             | 11       |
|    iterations      | 64       |
|    time_elapsed    | 11904    |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=131500, episode_reward=2.80 +/- 90.23
Episode length: 8.50 +/- 1.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 8.5        |
|    mean_reward          | 2.8        |
| time/                   |            |
|    total timesteps      | 131500     |
| train/                  |            |
|    approx_kl            | 0.07239984 |
|    clip_fraction        | 0.316      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.71      |
|    explained_variance   | 0.458      |
|    learning_rate        | 0.0003     |
|    loss                 | 606        |
|    n_updates            | 640        |
|    policy_gradient_loss | 0.0098     |
|    value_loss           | 1.91e+03   |
----------------------------------------
Eval num_timesteps=132000, episode_reward=122.85 +/- 34.09
Episode length: 10.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 123      |
| time/              |          |
|    total timesteps | 132000   |
---------------------------------
Eval num_timesteps=132500, episode_reward=65.87 +/- 104.51
Episode length: 9.00 +/- 1.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 65.9     |
| time/              |          |
|    total timesteps | 132500   |
---------------------------------
Eval num_timesteps=133000, episode_reward=45.77 +/- 80.33
Episode length: 9.50 +/- 0.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.5      |
|    mean_reward     | 45.8     |
| time/              |          |
|    total timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 8.21     |
|    ep_rew_mean     | 21.3     |
| time/              |          |
|    fps             | 11       |
|    iterations      | 65       |
|    time_elapsed    | 12090    |
|    total_timesteps | 133120   |
---------------------------------
Eval num_timesteps=133500, episode_reward=58.46 +/- 87.94
Episode length: 9.50 +/- 0.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9.5         |
|    mean_reward          | 58.5        |
| time/                   |             |
|    total timesteps      | 133500      |
| train/                  |             |
|    approx_kl            | 0.094842985 |
|    clip_fraction        | 0.337       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.728      |
|    explained_variance   | 0.388       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.14e+03    |
|    n_updates            | 650         |
|    policy_gradient_loss | 0.032       |
|    value_loss           | 2.05e+03    |
-----------------------------------------
Eval num_timesteps=134000, episode_reward=115.54 +/- 27.55
Episode length: 10.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 116      |
| time/              |          |
|    total timesteps | 134000   |
---------------------------------
Eval num_timesteps=134500, episode_reward=96.48 +/- 30.15
Episode length: 10.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 96.5     |
| time/              |          |
|    total timesteps | 134500   |
---------------------------------
Eval num_timesteps=135000, episode_reward=56.79 +/- 87.25
Episode length: 9.50 +/- 0.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.5      |
|    mean_reward     | 56.8     |
| time/              |          |
|    total timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.46     |
|    ep_rew_mean     | 58.9     |
| time/              |          |
|    fps             | 11       |
|    iterations      | 66       |
|    time_elapsed    | 12278    |
|    total_timesteps | 135168   |
---------------------------------
Eval num_timesteps=135500, episode_reward=6.11 +/- 102.02
Episode length: 9.00 +/- 1.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 9           |
|    mean_reward          | 6.11        |
| time/                   |             |
|    total timesteps      | 135500      |
| train/                  |             |
|    approx_kl            | 0.119328685 |
|    clip_fraction        | 0.404       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.805      |
|    explained_variance   | 0.234       |
|    learning_rate        | 0.0003      |
|    loss                 | 645         |
|    n_updates            | 660         |
|    policy_gradient_loss | 0.0543      |
|    value_loss           | 2.37e+03    |
-----------------------------------------
Eval num_timesteps=136000, episode_reward=65.11 +/- 55.10
Episode length: 9.75 +/- 0.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.75     |
|    mean_reward     | 65.1     |
| time/              |          |
|    total timesteps | 136000   |
---------------------------------
Eval num_timesteps=136500, episode_reward=103.86 +/- 21.32
Episode length: 10.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 104      |
| time/              |          |
|    total timesteps | 136500   |
---------------------------------
Eval num_timesteps=137000, episode_reward=52.13 +/- 89.23
Episode length: 9.50 +/- 0.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.5      |
|    mean_reward     | 52.1     |
| time/              |          |
|    total timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 8.96     |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 11       |
|    iterations      | 67       |
|    time_elapsed    | 12465    |
|    total_timesteps | 137216   |
---------------------------------
Eval num_timesteps=137500, episode_reward=87.45 +/- 94.54
Episode length: 9.50 +/- 0.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 9.5       |
|    mean_reward          | 87.5      |
| time/                   |           |
|    total timesteps      | 137500    |
| train/                  |           |
|    approx_kl            | 0.1560868 |
|    clip_fraction        | 0.422     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.773    |
|    explained_variance   | 0.344     |
|    learning_rate        | 0.0003    |
|    loss                 | 635       |
|    n_updates            | 670       |
|    policy_gradient_loss | 0.0473    |
|    value_loss           | 2.16e+03  |
---------------------------------------
Eval num_timesteps=138000, episode_reward=126.54 +/- 48.07
Episode length: 10.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 127      |
| time/              |          |
|    total timesteps | 138000   |
---------------------------------
Eval num_timesteps=138500, episode_reward=63.73 +/- 93.62
Episode length: 9.50 +/- 0.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.5      |
|    mean_reward     | 63.7     |
| time/              |          |
|    total timesteps | 138500   |
---------------------------------
Eval num_timesteps=139000, episode_reward=82.67 +/- 27.02
Episode length: 10.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 82.7     |
| time/              |          |
|    total timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.06     |
|    ep_rew_mean     | 35.6     |
| time/              |          |
|    fps             | 11       |
|    iterations      | 68       |
|    time_elapsed    | 12653    |
|    total_timesteps | 139264   |
---------------------------------
Eval num_timesteps=139500, episode_reward=41.40 +/- 92.86
Episode length: 8.00 +/- 3.46
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 8          |
|    mean_reward          | 41.4       |
| time/                   |            |
|    total timesteps      | 139500     |
| train/                  |            |
|    approx_kl            | 0.08512202 |
|    clip_fraction        | 0.363      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.766     |
|    explained_variance   | 0.291      |
|    learning_rate        | 0.0003     |
|    loss                 | 663        |
|    n_updates            | 680        |
|    policy_gradient_loss | 0.0539     |
|    value_loss           | 2.14e+03   |
----------------------------------------
Eval num_timesteps=140000, episode_reward=8.29 +/- 64.05
Episode length: 9.25 +/- 0.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.25     |
|    mean_reward     | 8.29     |
| time/              |          |
|    total timesteps | 140000   |
---------------------------------
Eval num_timesteps=140500, episode_reward=43.11 +/- 64.77
Episode length: 9.50 +/- 0.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.5      |
|    mean_reward     | 43.1     |
| time/              |          |
|    total timesteps | 140500   |
---------------------------------
Eval num_timesteps=141000, episode_reward=54.50 +/- 104.94
Episode length: 9.00 +/- 1.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | 54.5     |
| time/              |          |
|    total timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 8.61     |
|    ep_rew_mean     | -2.03    |
| time/              |          |
|    fps             | 11       |
|    iterations      | 69       |
|    time_elapsed    | 12839    |
|    total_timesteps | 141312   |
---------------------------------
Eval num_timesteps=141500, episode_reward=69.88 +/- 84.55
Episode length: 9.50 +/- 0.87
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 9.5      |
|    mean_reward          | 69.9     |
| time/                   |          |
|    total timesteps      | 141500   |
| train/                  |          |
|    approx_kl            | 0.093889 |
|    clip_fraction        | 0.31     |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.728   |
|    explained_variance   | 0.401    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.16e+03 |
|    n_updates            | 690      |
|    policy_gradient_loss | 0.0313   |
|    value_loss           | 2.05e+03 |
--------------------------------------
Eval num_timesteps=142000, episode_reward=39.79 +/- 90.96
Episode length: 9.25 +/- 1.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.25     |
|    mean_reward     | 39.8     |
| time/              |          |
|    total timesteps | 142000   |
---------------------------------
Eval num_timesteps=142500, episode_reward=68.26 +/- 81.14
Episode length: 9.50 +/- 0.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.5      |
|    mean_reward     | 68.3     |
| time/              |          |
|    total timesteps | 142500   |
---------------------------------
Eval num_timesteps=143000, episode_reward=-7.59 +/- 88.08
Episode length: 8.75 +/- 1.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.75     |
|    mean_reward     | -7.59    |
| time/              |          |
|    total timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 8.92     |
|    ep_rew_mean     | 10.3     |
| time/              |          |
|    fps             | 11       |
|    iterations      | 70       |
|    time_elapsed    | 13025    |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=143500, episode_reward=99.57 +/- 26.22
Episode length: 10.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 10         |
|    mean_reward          | 99.6       |
| time/                   |            |
|    total timesteps      | 143500     |
| train/                  |            |
|    approx_kl            | 0.10546102 |
|    clip_fraction        | 0.356      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.712     |
|    explained_variance   | 0.341      |
|    learning_rate        | 0.0003     |
|    loss                 | 721        |
|    n_updates            | 700        |
|    policy_gradient_loss | 0.0522     |
|    value_loss           | 1.72e+03   |
----------------------------------------
Eval num_timesteps=144000, episode_reward=-16.85 +/- 79.95
Episode length: 8.50 +/- 1.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.5      |
|    mean_reward     | -16.9    |
| time/              |          |
|    total timesteps | 144000   |
---------------------------------
Eval num_timesteps=144500, episode_reward=91.58 +/- 44.02
Episode length: 10.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 91.6     |
| time/              |          |
|    total timesteps | 144500   |
---------------------------------
Eval num_timesteps=145000, episode_reward=45.69 +/- 95.31
Episode length: 9.25 +/- 1.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.25     |
|    mean_reward     | 45.7     |
| time/              |          |
|    total timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.28     |
|    ep_rew_mean     | 54.3     |
| time/              |          |
|    fps             | 11       |
|    iterations      | 71       |
|    time_elapsed    | 13212    |
|    total_timesteps | 145408   |
---------------------------------
Eval num_timesteps=145500, episode_reward=22.80 +/- 81.41
Episode length: 9.00 +/- 1.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 9          |
|    mean_reward          | 22.8       |
| time/                   |            |
|    total timesteps      | 145500     |
| train/                  |            |
|    approx_kl            | 0.08891098 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.698     |
|    explained_variance   | 0.278      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.04e+03   |
|    n_updates            | 710        |
|    policy_gradient_loss | 0.047      |
|    value_loss           | 2.15e+03   |
----------------------------------------
Eval num_timesteps=146000, episode_reward=77.62 +/- 101.61
Episode length: 9.25 +/- 1.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.25     |
|    mean_reward     | 77.6     |
| time/              |          |
|    total timesteps | 146000   |
---------------------------------
Eval num_timesteps=146500, episode_reward=51.30 +/- 83.32
Episode length: 9.50 +/- 0.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.5      |
|    mean_reward     | 51.3     |
| time/              |          |
|    total timesteps | 146500   |
---------------------------------
Eval num_timesteps=147000, episode_reward=-37.16 +/- 85.26
Episode length: 8.00 +/- 1.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8        |
|    mean_reward     | -37.2    |
| time/              |          |
|    total timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.38     |
|    ep_rew_mean     | 67.9     |
| time/              |          |
|    fps             | 11       |
|    iterations      | 72       |
|    time_elapsed    | 13399    |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=147500, episode_reward=63.26 +/- 87.59
Episode length: 9.50 +/- 0.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 9.5        |
|    mean_reward          | 63.3       |
| time/                   |            |
|    total timesteps      | 147500     |
| train/                  |            |
|    approx_kl            | 0.07791408 |
|    clip_fraction        | 0.329      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.743     |
|    explained_variance   | 0.265      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.07e+03   |
|    n_updates            | 720        |
|    policy_gradient_loss | 0.042      |
|    value_loss           | 2.14e+03   |
----------------------------------------
Eval num_timesteps=148000, episode_reward=90.74 +/- 30.17
Episode length: 10.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 90.7     |
| time/              |          |
|    total timesteps | 148000   |
---------------------------------
Eval num_timesteps=148500, episode_reward=28.19 +/- 87.47
Episode length: 9.25 +/- 0.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.25     |
|    mean_reward     | 28.2     |
| time/              |          |
|    total timesteps | 148500   |
---------------------------------
Eval num_timesteps=149000, episode_reward=-5.46 +/- 81.64
Episode length: 9.00 +/- 1.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9        |
|    mean_reward     | -5.46    |
| time/              |          |
|    total timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=-4.64 +/- 121.44
Episode length: 8.25 +/- 1.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 8.25     |
|    mean_reward     | -4.64    |
| time/              |          |
|    total timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.64     |
|    ep_rew_mean     | 71       |
| time/              |          |
|    fps             | 11       |
|    iterations      | 73       |
|    time_elapsed    | 13588    |
|    total_timesteps | 149504   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-0.02 +/- 89.73
Episode length: 8.50 +/- 1.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 8.5        |
|    mean_reward          | -0.0181    |
| time/                   |            |
|    total timesteps      | 150000     |
| train/                  |            |
|    approx_kl            | 0.13668343 |
|    clip_fraction        | 0.376      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.743     |
|    explained_variance   | 0.312      |
|    learning_rate        | 0.0003     |
|    loss                 | 785        |
|    n_updates            | 730        |
|    policy_gradient_loss | 0.0379     |
|    value_loss           | 2.08e+03   |
----------------------------------------
Eval num_timesteps=150500, episode_reward=96.44 +/- 25.41
Episode length: 10.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 96.4     |
| time/              |          |
|    total timesteps | 150500   |
---------------------------------
Eval num_timesteps=151000, episode_reward=45.10 +/- 82.21
Episode length: 9.50 +/- 0.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 9.5      |
|    mean_reward     | 45.1     |
| time/              |          |
|    total timesteps | 151000   |
---------------------------------
Eval num_timesteps=151500, episode_reward=102.15 +/- 40.85
Episode length: 10.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 10       |
|    mean_reward     | 102      |
| time/              |          |
|    total timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.6      |
|    ep_rew_mean     | 54.8     |
| time/              |          |
|    fps             | 11       |
|    iterations      | 74       |
|    time_elapsed    | 13775    |
|    total_timesteps | 151552   |
---------------------------------

(ppo_drone) F:\RL Final\PPO-based-Autonomous-Navigation-for-Quadcopters-main>  